# -*- coding: utf-8 -*-
"""UpgradedVersion.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1giMoCN_25nq_4EZm4BLASt3CH2LmykQS
"""

from google.colab import drive
drive.mount('/content/drive')

data_file_path = "drive/My Drive/Colab Notebooks/data/Historical_Train_Operation_Dataset/combined_Final.xlsx"
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
data_table = pd.read_excel(data_file_path, "combined_Final")
data_onehot_route = pd.read_excel(data_file_path, "combined_Final", usecols = "P:FK", sep = "\t")
data_labels = pd.read_excel(data_file_path, "combined_Final", usecols = "B", squeeze = "True")
data_train_index = pd.read_excel(data_file_path, "combined_Final", usecols = "A")
data_dep_time = pd.read_excel(data_file_path, "combined_Final", usecols = "C", squeeze = "True")
data_arr_time = pd.read_excel(data_file_path, "combined_Final", usecols = "D", squeeze = "True")
data_margin_time = pd.read_excel(data_file_path, "combined_Final", usecols = "E")
data_speed_limit = pd.read_excel(data_file_path, "combined_Final", usecols = "F")
data_passenger_flow = pd.read_excel(data_file_path, "combined_Final", usecols = "FL")
data_onehot_rolling_stock = pd.read_excel(data_file_path, "combined_Final", usecols = "G:O")


data_route = np.array(data_onehot_route)
list_data_labels = data_labels.values.tolist()
list_data_train_index = data_train_index.values.tolist()
list_data_dep_time = data_dep_time.values.tolist()
list_data_arr_time = data_arr_time.values.tolist()
list_data_margin_time = data_margin_time.values.tolist()
list_data_speed_limit = data_speed_limit.values.tolist()
list_data_passenger_flow = data_passenger_flow.values.tolist()
list_data_onehot_rolling_stock = data_onehot_rolling_stock.values.tolist()
print(list_data_labels)
print(list_data_dep_time)
print(list_data_arr_time)
print(data_table)

def categorize_labels(l):
    categorized_labels = []
    for i in l:
        if i == 0.0:
            i = "0"
        elif i >= 1.0 and i <= 6.0:
            i = "1"
        elif i > 6.0 and i <= 11.0:
            i = "2"
        elif i > 11.0 and i <= 16.0:
            i = "3"
        else:
            i = "4"
        categorized_labels.append(i)
    return categorized_labels

def remove_comma(o):
    onehot_code_after_removed_comma = []
    for i in o:
        onehot_code = []
        onehot_code.append("".join("%s"%id for id in i))
        onehot_code_after_removed_comma.append(onehot_code)
    return onehot_code_after_removed_comma

categorized_labels_list = categorize_labels(list_data_labels)
print(categorized_labels_list.count("0"))
print(categorized_labels_list.count("1"))
print(categorized_labels_list.count("2"))
print(categorized_labels_list.count("3"))
print(categorized_labels_list.count("4"))

removed_comma_onehot_list = remove_comma(list_data_onehot_rolling_stock)
print(removed_comma_onehot_list)

def calculate_time_hour_minute(t):
    time_minute_sin = []
    time_minute_cos = []
    time_hour_sin = []
    time_hour_cos = []
    for i in t:
        minute = int(i) % 60
        hour = (int(i) - minute) / 60
        minute_sin = np.sin(minute*(2.*np.pi/24))
        minute_cos = np.cos(minute*(2.*np.pi/24))
        hour_sin = np.sin(hour*(2.*np.pi/24))
        hour_cos = np.cos(hour*(2.*np.pi/24))
        time_minute_sin.append(minute_sin)
        time_minute_cos.append(minute_cos)
        time_hour_sin.append(hour_sin)
        time_hour_cos.append(hour_cos)
    return time_minute_sin, time_minute_cos, time_hour_sin, time_hour_cos

def limit_time_in_24_hours(time):
    time_24_hours = []
    for i in time:
        if i > 1440:
            i = i - 1440
            time_24_hours.append(i)
        else:
            time_24_hours.append(i)
    return time_24_hours

from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
scaler.fit(data_route)
standarized_data_route = scaler.transform(data_route)
Pca = PCA(n_components = 50)
Pca.fit(standarized_data_route)
print(Pca.explained_variance_ratio_)
print(Pca.explained_variance_)
low_dimen_route = []
low_dimen_route = Pca.transform(standarized_data_route)
print(low_dimen_route)
print(low_dimen_route.shape)

from collections import Counter
list_data_dep_time = limit_time_in_24_hours(list_data_dep_time)
list_data_arr_time = limit_time_in_24_hours(list_data_arr_time)
dep_time_minute_sin, dep_time_minute_cos, dep_time_hour_sin, dep_time_hour_cos = calculate_time_hour_minute(list_data_dep_time)
arr_time_minute_sin, arr_time_minute_cos, arr_time_hour_sin, arr_time_hour_cos = calculate_time_hour_minute(list_data_arr_time)

from sklearn import preprocessing

dep_time_minute_sin = np.array(dep_time_minute_sin).reshape(-1,1)
#scaler_1 = preprocessing.StandardScaler().fit(dep_time_minute_sin)
#dep_time_minute_sin = scaler_1.transform(dep_time_minute_sin)

dep_time_minute_cos = np.array(dep_time_minute_cos).reshape(-1,1)
#scaler_2 = preprocessing.StandardScaler().fit(dep_time_minute_cos)
#dep_time_minute_cos = scaler_2.transform(dep_time_minute_cos)

dep_time_hour_sin = np.array(dep_time_hour_sin).reshape(-1,1)
#scaler_3 = preprocessing.StandardScaler().fit(dep_time_hour_sin)
#dep_time_hour_sin = scaler_3.transform(dep_time_hour_sin)

dep_time_hour_cos = np.array(dep_time_hour_cos).reshape(-1,1)
#scaler_4 = preprocessing.StandardScaler().fit(dep_time_hour_cos)
#dep_time_hour_cos = scaler_4.transform(dep_time_hour_cos)

arr_time_minute_sin = np.array(arr_time_minute_sin).reshape(-1,1)
#scaler_5 = preprocessing.StandardScaler().fit(arr_time_minute_sin)
#arr_time_minute_sin = scaler_5.transform(arr_time_minute_sin)

arr_time_minute_cos = np.array(arr_time_minute_cos).reshape(-1,1)
#scaler_6 = preprocessing.StandardScaler().fit(arr_time_minute_cos)
#arr_time_minute_cos = scaler_6.transform(arr_time_minute_cos)

arr_time_hour_sin = np.array(arr_time_hour_sin).reshape(-1,1)
#scaler_7 = preprocessing.StandardScaler().fit(arr_time_hour_sin)
#arr_time_hour_sin = scaler_7.transform(arr_time_hour_sin)

arr_time_hour_cos = np.array(arr_time_hour_cos).reshape(-1,1)
#scaler_8 = preprocessing.StandardScaler().fit(arr_time_hour_cos)
#arr_time_hour_cos = scaler_8.transform(arr_time_hour_cos)

from sklearn import preprocessing

enc=preprocessing.OneHotEncoder()
enc.fit(data_speed_limit)
data_speed_limit_one_hot = enc.transform(data_speed_limit).toarray()
scaler_9 = preprocessing.StandardScaler().fit(data_speed_limit_one_hot)
data_speed_limit_one_hot_standardized = scaler_9.transform(data_speed_limit_one_hot)

enc.fit(removed_comma_onehot_list)
data_rolling_stock_one_hot = enc.transform(removed_comma_onehot_list).toarray()
scaler_10 = preprocessing.StandardScaler().fit(data_rolling_stock_one_hot)
data_rolling_stock_one_hot_standardized = scaler_10.transform(data_rolling_stock_one_hot)

scaler_11 = preprocessing.StandardScaler().fit(data_margin_time)
data_margin_time_standardized = scaler_11.transform(data_margin_time)

scaler_12 = preprocessing.StandardScaler().fit(data_passenger_flow)
data_passenger_flow_standardized = scaler_12.transform(data_passenger_flow)

print(data_margin_time_standardized)
print(data_speed_limit_one_hot)

result = np.concatenate((dep_time_minute_sin, dep_time_minute_cos), axis = 1)
result = np.concatenate((result, dep_time_hour_sin), axis = 1)
result = np.concatenate((result, dep_time_hour_cos), axis = 1)
result = np.concatenate((result, arr_time_minute_sin), axis = 1)
result = np.concatenate((result, arr_time_minute_cos), axis = 1)
result = np.concatenate((result, arr_time_hour_sin), axis = 1)
result = np.concatenate((result, arr_time_hour_cos), axis = 1)

result = np.concatenate((result, data_margin_time_standardized), axis = 1)
result = np.concatenate((result, data_speed_limit_one_hot_standardized), axis = 1)
result = np.concatenate((result, data_rolling_stock_one_hot_standardized), axis = 1)
result = np.concatenate((result, data_passenger_flow_standardized), axis = 1)
result = np.concatenate((result, low_dimen_route), axis = 1)

print(result[0])

from sklearn.preprocessing import LabelEncoder
y = np.array(categorized_labels_list)
y = np.ravel(y)
labelencoder = LabelEncoder()
y = labelencoder.fit_transform(y)



#import seaborn as sns
#X_res = pd.DataFrame (result)
#print (X_res)
#corr = X_res.corr()
#sns.heatmap(corr)

counter = Counter(y)
print(counter)

from imblearn.over_sampling import SMOTE 
oversample = SMOTE(random_state=20)
X_res, y_res = oversample.fit_resample(result, y)

counter = Counter(y_res)
print(counter)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size = 0.2)

from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import MultinomialNB
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neural_network import MLPClassifier
from sklearn.svm import LinearSVC
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import cross_val_score
from sklearn.metrics import accuracy_score
from sklearn.metrics import f1_score
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
from sklearn.metrics import plot_confusion_matrix

class_names = ["0", "1", "2", "3", "4"]

###decesion tree
par1 = {'criterion':['gini','entropy']}
clf_tree = DecisionTreeClassifier(max_depth = None, min_samples_split=5)
clf_tree = GridSearchCV(clf_tree, par1, cv= 5, n_jobs=10, scoring = 'f1_macro')
scores_1 = cross_val_score(clf_tree, X_train, y_train, cv=5)
clf_tree.fit(X_train, y_train)
clf_tree = clf_tree.best_estimator_.fit(X_train, y_train)
print(scores_1)
print("the accuracy score on testset by using Decesion Tree")
print(accuracy_score(clf_tree.predict(X_test), y_test))
print(f1_score(clf_tree.predict(X_test), y_test, average='macro'))
conf_mat_1 = confusion_matrix(y_test, clf_tree.predict(X_test))
print(conf_mat_1)
titles_options = [("Confusion matrix, without normalization", None),("Normalized confusion matrix", 'true')]
for title, normalize in titles_options:
    disp = plot_confusion_matrix(clf_tree, X_test, y_test, display_labels=class_names, cmap=plt.cm.Blues, normalize=normalize)
    disp.ax_.set_title(title)
    print(title)
    print(disp.confusion_matrix)

plt.show()

###Random Forest
par3 = {'n_estimators':[100, 110, 120, 130, 140, 150, 200, 250, 300, 350, 400, 450, 500]}
clf_rt = RandomForestClassifier()
clf_rt = GridSearchCV(clf_rt, par3, cv=5, n_jobs=10, scoring = 'f1_macro')
scores_2 = cross_val_score(clf_rt, X_train, y_train, cv=5)
clf_rt.fit(X_train, y_train)
clf_rt = clf_rt.best_estimator_.fit(X_train, y_train)
print(scores_2)
print("the accuracy score on testset by using Random Forest")
print(accuracy_score(clf_rt.predict(X_test), y_test))
print(f1_score(clf_rt.predict(X_test), y_test, average='macro'))
conf_mat_2 = confusion_matrix(y_test, clf_rt.predict(X_test))
print(conf_mat_2)

titles_options = [("Confusion matrix, without normalization", None),("Normalized confusion matrix", 'true')]
for title, normalize in titles_options:
    disp = plot_confusion_matrix(clf_rt, X_test, y_test, display_labels=class_names, cmap=plt.cm.Blues, normalize=normalize)
    disp.ax_.set_title(title)
    print(title)
    print(disp.confusion_matrix)

plt.show()

eclf1 = VotingClassifier(estimators=[('dt', clf_tree), ('rf', clf_rt)], voting='hard')
eclf1 = eclf1.fit(X_train, y_train)
print("the accuracy score on testset by using Voting rules")
print(accuracy_score(eclf1.predict(X_test), y_test))
print(f1_score(eclf1.predict(X_test), y_test, average='macro'))
conf_mat_4 = confusion_matrix(y_test, eclf1.predict(X_test))
print(conf_mat_4)

clf = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(500, ), random_state=1)
clf.fit(X_train, y_train)
scores_3 = cross_val_score(clf, X_train, y_train, cv=5)
print("the accuracy score on testset by using NN")
print(accuracy_score(clf.predict(X_test), y_test))
print(f1_score(clf.predict(X_test), y_test, average='macro'))
conf_mat_5 = confusion_matrix(y_test, clf.predict(X_test))
print(conf_mat_4)

titles_options = [("Confusion matrix, without normalization", None),("Normalized confusion matrix", 'true')]
for title, normalize in titles_options:
    disp = plot_confusion_matrix(clf, X_test, y_test, display_labels=class_names, cmap=plt.cm.Blues, normalize=normalize)
    disp.ax_.set_title(title)
    print(title)
    print(disp.confusion_matrix)

plt.show()

clf = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(600,), random_state=1)
clf.fit(X_train, y_train)
scores_3 = cross_val_score(clf, X_train, y_train, cv=5)
print(scores_3)
print("the accuracy score on testset by using NN")
print(accuracy_score(clf.predict(X_test), y_test))
print(f1_score(clf.predict(X_test), y_test, average='macro'))
conf_mat_5 = confusion_matrix(y_test, clf.predict(X_test))
print(conf_mat_5)

clf.fit(X_train, y_train)
scores_4 = cross_val_score(clf, X_train, y_train, cv=5)
print(scores_4)
print("the accuracy score on testset by using NN")
print(accuracy_score(clf.predict(X_test), y_test))
print(f1_score(clf.predict(X_test), y_test, average='macro'))
conf_mat_6 = confusion_matrix(y_test, clf.predict(X_test))
print(conf_mat_6)